{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_network_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Barttje/summercourse-ml/blob/master/neural_network_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt3QsZrmpHsM",
        "colab_type": "text"
      },
      "source": [
        "# Codecentric Summercourse ML \n",
        "This is part one of the summercourse ML organised by Codecentric. \n",
        "## Neural network\n",
        "In this tutorial we are going to explain how to create a neural network in Keras that is used for classifying the MNIST fashion data set. This is a dataset of 60.000 Zalando's article images.\n",
        "Each training and test example is assigned to one of the following labels:\n",
        "0. T-Shirt\n",
        "1. Trouser\n",
        "2. Pullover\n",
        "3. Dress\n",
        "4. Coat\n",
        "5. Sandal\n",
        "6. Shirt\n",
        "7. Sneaker\n",
        "8. Bag\n",
        "9. Ankle boot \n",
        "\n",
        "More information about the data set can be found [here](https://www.kaggle.com/zalando-research/fashionmnist).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feiNW8bqyI1c",
        "colab_type": "text"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJX7HcvQpFyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmfihgWCyESp",
        "colab_type": "text"
      },
      "source": [
        "#Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voxKUe00uvtq",
        "colab_type": "code",
        "outputId": "97a6c7c9-53f1-4925-98c5-6bf8bb7d649b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels = {0:\"T-Shirt\", 1:\"Trouser\", 2:\"Pullover\", 3:\"Dress\", 4:\"Coat\",5:\"Sandal\",6:\"Shirt\",7:\"Sneaker\",8:\"Bag\",9:\"Ankle boot\"}\n",
        "print(labels[2])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pullover\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVpnfu_rAqK7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Create a mapper to translate the labels to the actual values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihNPU7HyIlUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train_input, y_train_input), (x_test_input, y_test_input) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYR2Uo4rKGwi",
        "colab_type": "text"
      },
      "source": [
        "The load_data() function on the dataset can be used for the datasets that are included in Keras. The other datasets that can be loaded in the same way can be found on the [Keras website](https://keras.io/datasets/). \n",
        "The load_data() function returns the train and test data set for the MNIST fashion data set. Both the test and train data are divided in two parts, the image and the label. x_train_input contains all the images we are going to use to train the neural network. The y_train_input contains all the labels for those images. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu1IyLtRI_Ga",
        "colab_type": "code",
        "outputId": "44d9357b-be54-452c-97d7-e0c6ecd0d025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "image_index = 8921 # Can be any number between 0 and 59999\n",
        "print(y_train_input[image_index], labels[y_train_input[image_index]])\n",
        "plt.imshow(x_train_input[image_index])\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Trouser\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff27c273cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgeVD46YKAXb",
        "colab_type": "text"
      },
      "source": [
        "The *matplotlib.pyplot as plt* can be used to show the images in the dataset. \n",
        "We also print the label of that image, which is a number between 1 and 9. With the mapping we can convert this number to the actual name of the label. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRuWSJFxP4c4",
        "colab_type": "code",
        "outputId": "6dbe4ca6-5ba6-4d42-9e81-85b9fe0bc7f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(x_train_input.shape, y_train_input.shape, x_test_input.shape, y_test_input.shape)\n",
        "print(x_train_input[8921][26][22])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n",
            "229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dMGDTALRXE4",
        "colab_type": "text"
      },
      "source": [
        "In machine learning it is really important to understand and have a feeling for the shapes of the data. \n",
        " We need to make sure the input data matches the first layer of the model.  Above we printed the shape of each data. For x_train_input this is (60000, 28, 28). This means we have a three-dimensional array that contains 60.000 * 28 * 28 data points (~47 million). The 60.000 are the number of images we have in the dataset. The 28, 28 are the height and the width of the images in pixels. Each datapoint is a number between 0 and 255, which represents the color of that pixel. \n",
        "\n",
        "The y_train_input is (60000,) which is a array with 60.000 datapoints. Each datapoint represent the label of the image and is a number between 0 and 9. \n",
        "\n",
        "x_test_input and y_test_input are similar to x_train_input and y_train_input, only with a different amount of images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ73y917P8pB",
        "colab_type": "code",
        "outputId": "3305c628-2d08-401f-bd66-c80eeda33d93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "image_size = 28*28\n",
        "# x_train.shape[0] is the amount of images. \n",
        "x_train = x_train_input.reshape(x_train_input.shape[0], image_size) # Transform from matrix to vector\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255 # Normalize inputs from 0-255 to 0.0-1.0\n",
        "\n",
        "x_test = x_test_input.reshape(x_test_input.shape[0], image_size) # Transform from matrix to vector\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255 # Normalize inputs from 0-255 to 0.0-1.0\n",
        "\n",
        "print(x_train.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSYNQsx7TZh-",
        "colab_type": "text"
      },
      "source": [
        "The neural network that we are creating is not expecting two dimensional data. (*There are layers that accept two dimensional arrays, which we will see in the next part*) So we have translate the input data into a format the neural network can handle. We can use the reshape function to change the format from a (60000, 28, 28) to a  (60000, 784) array. Furthermore we normalize the data between 0.0 and 1.0, by dividing by 255. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhQ52etaWwle",
        "colab_type": "code",
        "outputId": "5aaaf39d-454f-444b-de88-d039f7e3d1d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train_input, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test_input, num_classes)\n",
        "print(y_train.shape, y_test.shape)\n",
        "\n",
        "index_y = 3\n",
        "print(y_train[index_y])\n",
        "print(y_train_input[index_y])\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 10) (10000, 10)\n",
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J-drBQ_ZN-Q",
        "colab_type": "text"
      },
      "source": [
        "We also have to change the format of the labels, since the numerical values of the labels have no relation with each other. To do this, we can use the keras utils to map each label to a single column. Each Shirt (0), has 1 in the first column. All other labels have 0 in that column. The model will predict the likelyhood for each category whether that is the item in the picture.\n",
        "\n",
        "\n",
        "![wow](https://github.com/Barttje/summercourse-ml/blob/master/one_hot_encoding.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLJJOvYKyQtT",
        "colab_type": "text"
      },
      "source": [
        "#Creating and training the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnOSVRnFYKH6",
        "colab_type": "code",
        "outputId": "fa3d98f6-e11d-4b42-cc3a-32f549dc01b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "neurons_in_hidden_layer=512\n",
        "model = Sequential()\n",
        "model.add(Dense(units=neurons_in_hidden_layer, activation='relu', input_shape=(image_size,))) # Input layer - > Hidden Layer\n",
        "model.add(Dense(units=num_classes, activation='softmax')) # Hidden Layer -> Output layer\n",
        "model.compile(optimizer=\"adam\", loss='categorical_crossentropy' metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "print(\"input: \", model.input)\n",
        "print(\"output: \", model.output)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_19 (Dense)             (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "input:  Tensor(\"dense_19_input:0\", shape=(?, 784), dtype=float32)\n",
            "output:  Tensor(\"dense_20/Softmax:0\", shape=(?, 10), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDyP7Nz3ZKu3",
        "colab_type": "text"
      },
      "source": [
        "So here we have defined three layers in the model. Although it looks like we only added two layers to the model, it is the connection between two layers that are added to the model. So in our model, the first Dense layer is the connection between the input layer and the hidden layer. The second Dense layer is the connection between the hidden layer and the output layer.  ![alt text](https://raw.githubusercontent.com/Barttje/summercourse-ml/master/Neural_network_fashion_1.png)\n",
        "The following will print the input and the output the model expects. \n",
        "```\n",
        "print(\"input: \", model.input)\n",
        "print(\"output: \", model.input)\n",
        "\n",
        "```\n",
        "\n",
        "In our case it will print the following: \n",
        "\n",
        "*input:  Tensor(\"dense_19_input:0\", shape=(?, 784), dtype=float32)*\n",
        "\n",
        "*output:  Tensor(\"dense_20/Softmax:0\", shape=(?, 10), dtype=float32)*\n",
        "\n",
        "The input for the model is shape *(?, 784)*, which means a two dimensional array where the second dimension has 784 values. It does not matter how many images we have, but they have to be of size 784. Which has to match the shape of *X_training* and *X_test*\n",
        "\n",
        "The output of the model is shape *(?,10)*, which means we get 10 values for each image we put in the model. A prediction for each possible item, which has to match the shape of our *y_training* and *y_test*.\n",
        "\n",
        "The units are the outputs of one layer. So the first dense layer has as input the image size, and as output 512 values. This also means that the second Dense layer has as input the 512 values, which we do not have to configure. As output ( units) for this layers we define the number of classes. This has to match the shape of the \n",
        "\n",
        "The other parameters for the models are the activation function, which is usually *relu*, but when you are dealing with a classification problem the *softmax* activation function is a good idea for the last layer. More information about possible activation functions in Keras can be found [here](https://keras.io/activations/). More information about the theory behind activation functions can be found [here](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0). \n",
        "\n",
        "We can also configure the optimizer, which is the algortihm that determines the weights during training. *adam* is the most popular one, so we use that one. More information about the possible optimizers in Keras can be found [here](https://keras.io/optimizers/) and more information about the theory behind optizimers can be found [here](https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f). \n",
        "\n",
        "Finally we configure the loss function during training. The loss functions measures how well the model is working on the data set. One of the most popular ones is the mean squared error. However for classification problems that predict 0 or 1, the *categorical_crossentropy* works better. All the possible options for the loss functions in Keras can be found [here](https://keras.io/losses/) and a [great article ](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)explaining more about which one to choose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHjoWHgCYUyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 10\n",
        "history = model.fit(x_train, y_train, validation_split=0.2, epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j50yPJq6ZFY4",
        "colab_type": "text"
      },
      "source": [
        "explain epochs and validation split \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_9m3bWmYzZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy  = model.evaluate(x_test, y_test)\n",
        "print(loss, accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zueKzFx_Y-0L",
        "colab_type": "text"
      },
      "source": [
        "explain loss and accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsQl05l3yXDW",
        "colab_type": "text"
      },
      "source": [
        "#Applying the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIgN8-_4ZRNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = 6293\n",
        "pred = model.predict(x_test[-index:-index+1]) \n",
        "print(pred)\n",
        "print(pred.argmax(), labels[pred.argmax()])\n",
        "print(y_test[-index:-index+1].argmax(), labels[y_test[-index:-index+1].argmax()] )\n",
        "plt.imshow(x_test[-index:-index+1][0].reshape(28,28))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laSiVkUBeCpT",
        "colab_type": "text"
      },
      "source": [
        "explain prediction, explain arg max, compare with actual class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw49O5z7wGtt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list all data in history\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04-OVVJ9GAmQ",
        "colab_type": "text"
      },
      "source": [
        "##Overfitting and Underfitting\n",
        "Overfitting occurs when the model also learns from the noise and inaccurate data. Underfitting occurs when the model does not learn anything at all. With overfitting you learn to much from the data and with underfitting you learn not enough from the data. ![Overfitting,underfitting and balanced](https://github.com/Barttje/summercourse-ml/blob/master/Overfitting_underfitting.png?raw=true)\n",
        "### Detecting overfitting/underfitting\n",
        "Underfitting occurs when the loss is very high and the accuray is very low. Overfitting occurs when there is a gap between the accuracy and loss of the training data set and of the test data set. \n",
        "#### Handling overfitting\n",
        "Early Stopping: Its rules provide us the guidance as to how many iterations can be run before learner begins to over-fit.\n",
        "Drop out layers: \n",
        "#### Handling underfitting\n",
        "Try to get more input data. \n",
        "Try more epochs. Check if score is increasing. \n",
        "If there is high bias, try to increase the complexity of the model, by adding more layers. "
      ]
    }
  ]
}